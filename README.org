#+STARTUP: fold indent
#+OPTIONS: tex:t toc:2 H:6 ^:{}
#+ODT_STYLES_FILE: "/Users/gilgamesh/Google Drive/Templates/styles.xml"

#+TITLE: Programming in Python
#+AUTHOR: Derek Devnich

* *WEEK 1: Fundamentals*
* Orientation
** What programming language should I use?
1. Use the language that your friends use (so you can ask them for help)
2. Use a language that has a community of practice for your desired use case (you can find documentation, bug reports, sample code, etc.)
3. Use a language that is "best" by some technical definition

** Python is pretty good at lots of things
- "Glue" language intended to replace shell and Perl
- Concise, readable, good for rapid prototyping
- Access to linear algebra libraries in FORTRAN/C → user-friendly numeric computing

** Literate programming and notebooks
- Blend code, documentation, and visualization
- Good for trying things, demos
- Bad for massive or long-running processes
- You can export notebooks as .py files when they outgrow the notebook format

* Jupyter commands
** How to start Jupyter Lab
1. Method 1
   1. Open Anaconda Navigator
   2. Run Jupyter Lab

2. Method 2
   Open Terminal (MacOS/Linux) or Anaconda Prompt (Windows)
   #+BEGIN_SRC bash
   cd Desktop/data
   jupyter lab
   #+END_SRC

** Navigation
- Navigate to where you want to be before creating new notebook
- Rename your notebook to something informative
- Use drag-and-drop interface to move .ipynb file to new location

** Writing code
1. Execute cell with CTRL-Enter
   #+BEGIN_SRC python
   3 + 7
   #+END_SRC

2. Execute cell and move to new cell with Shift-Enter
   #+BEGIN_SRC python
   # This is a comment
   print("hello")
   #+END_SRC

3. Cells can be formatted as Code or Markdown

4. Many keyboard shortcuts are available; see https://gist.github.com/discdiver/9e00618756d120a8c9fa344ac1c375ac

5. (Optional) Jupyter Lab undestands (some) terminal commands
   #+BEGIN_SRC bash
   ls
   #+END_SRC

6. (Optional) Jupyter Lab (IPython, actually) has "magic" commands that start with ~%~ (line) or ~%%~ (cell)
   #+BEGIN_SRC python
   # Print current items in memory
   %dirs

   # Get environment variables
   %env

   # Cell magic: Run bash in a subprocess
   %%bash

   # Cell magic: Time cell execution
   %%time
   #+END_SRC

   - The magic command must be in the *first line* of the cell (no comments)
   - Some commands are not available on Windows (e.g. ~%%bash~)

* Variables and Assignment
** Use variables to store values
Variables are names for values.
#+BEGIN_SRC python
first_name = 'Derek'
age = 42
#+END_SRC

** Rules for naming things
1. Can only contain letters, digits, and underscore
2. Cannot start with a digit
3. Are case sensitive: ~age~, ~Age~ and ~AGE~

** Use ~print()~ to display values
#+BEGIN_SRC python
print(first_name, 'is', age, 'years old')
#+END_SRC

- Functions are verbs
- Functions end in ~()~
- Functions take arguments (i.e. they do stuff with the values that you give them)
- ~print()~ useful for tracking progress, debugging

** Jupyter Lab will always echo the last value in a cell
1. Python will evaluate and echo the last item
   #+BEGIN_SRC python
   first_name
   age
   #+END_SRC

2. If you want to see multiple items, you should explicitly print them
   #+BEGIN_SRC python
   print(first_name)
   print(age)
   #+END_SRC

** (Optional) Variables must be created before they are used
#+BEGIN_SRC python
# Prints an informative error message; more about this later
print(last_name)
#+END_SRC

** Variables can be used in calculations
#+BEGIN_SRC python
print(age)
age = age + 3
print(age)
#+END_SRC

** Variables only change value when something is assigned to them
Order of operations matters!
#+BEGIN_SRC python
first = 1
second = 5 * first
first = 2
print('first:', first)
print('second:', second)
#+END_SRC

* Data Types and Type Conversion
** Every value has a type
Most data is text and numbers, but there are many other types.
1. Integers: whole numbers (counting)
2. Floats: real numbers (math)
3. Strings: text
4. Files
5. Various collections (lists, sets, dictionaries, data frames, arrays)
6. More abstract stuff (e.g., database connection)

** The type determine what operations you can perform with a given value
1. Example 1: Subtraction makes sense for some kinds of data but not others
   #+BEGIN_SRC python
   print(5 - 3)
   print('hello' - 'h')
   #+END_SRC

3. Example 2: Some things have length and some don't
   Note that we can put functions inside other functions!
   #+BEGIN_SRC python
   print(len('hello'))
   print(len(5))
   #+END_SRC

** Use the built-in function ~type()~ to find the type of a value
1. Two types of number
   #+BEGIN_SRC python
   print(type(53))
   print(type(3.12))
   #+END_SRC

2. You can check the type of a variable
   #+BEGIN_SRC python
   fitness = 'average'
   type(fitness)
   #+END_SRC

3. Python is strongly-typed: It will (mostly) refuse to convert things automatically. The exception is mathematical operations with integers and floats.
   #+BEGIN_SRC python
   int_sum = 3 + 4
   mixed_sum = 3 + 4.0

   print(type(int_sum))
   print(type(mixed_sum))
   #+END_SRC

** You can explicitly convert data to a different type
1. Can't do math with text
   #+BEGIN_SRC python
   1 + '2'
   #+END_SRC

2. If you have string data, you can explicitly convert it to numeric data...
   #+BEGIN_SRC python
   print(1 + float('2'))
   print(1 + int('2'))
   #+END_SRC

3. ...and vice-versa
   #+BEGIN_SRC python
   text = str(3)

   print(text)
   print(type(text))
   #+END_SRC

4. What's going on under the hood?
   1. ~int~, ~float~, and ~str~ are types. More precisely, they are /classes/.
   2. ~int()~, ~float()~, and ~str()~ are functions that create new /instances/ of their respective classes. The argument to the creation function (e.g., ~'2'~) is the raw material for creating the new instance.

5. This can work for more complex data types as well, e.g. Pandas data frames and Numpy arrays.

** *Challenge*: Explain what each operator does
#+BEGIN_SRC python
# Floor
print('5 // 3:', 5 // 3)

# Floating point
print('5 / 3:', 5 / 3)

# Modulus (remainder)
print('5 % 3:', 5 % 3)
#+END_SRC

* Built-in Functions and Help
** How do we find out what's possible?
- Python.org tutorial
- Standard library reference (we will discuss libraries in the next section)
- References section of this document
- Stack Overflow

** (Optional) Use comments to add documentation to programs
Leave notes for Future You about what you've learned and how your code works.
#+BEGIN_SRC python
# This line isn't executed by Python
print("This cell has many comments")   # The rest of this line isn't executed either
#+END_SRC

** A function may take zero or more arguments
#+BEGIN_SRC python
print('before')
print()
print('after')
#+END_SRC

** Functions can have optional arguments
#+BEGIN_SRC python
# By default, we round to the nearest integer
round(3.712)
#+END_SRC

#+BEGIN_SRC python
# You can optionally specify the number of significant digits
round(3.712, 1)
#+END_SRC

** Use the built-in function ~help()~ to get help for a function
1. View the documentation for ~round()~
   #+BEGIN_SRC python
   help(round)
   #+END_SRC
   - 1 mandatory argument
   - 1 optional argument with a default value: ~ndigits=None~

2. You can proved arguments implicitly by order, or explicitly in any order
   #+BEGIN_SRC python
   # You can optionally specify the number of significant digits
   round(4.712823, ndigits=2)
   #+END_SRC

** Every function returns something
1. Collect the results of a function in a new variable. This is one of the ways we build complex programs.
   #+BEGIN_SRC python
   # You can optionally specify the number of significant digits
   rounded_num = round(4.712823, ndigits=2)
   print(rounded_num)
   #+END_SRC

   #+BEGIN_SRC python
   result = len("hello")
   print(result)
   #+END_SRC

2. (Optional) Some function only have "side effects"; they return ~None~
   #+BEGIN_SRC python
   result = print("hello")
   print(result)
   print(type(result))
   #+END_SRC

** (Optional) Functions will typically generalize in sensible ways
1. ~max()~ and ~min()~ do the intuitively correct thing with numerical and text data
   #+BEGIN_SRC python
   print(max(1, 2, 3))
   print(min('a', 'A', '0'))       # sort order is 0-9, A-Z, a-z
   #+END_SRC

2. Mixed numbers and text aren't meaningfully comparable
   #+BEGIN_SRC python
   max(1, 'a')
   #+END_SRC

** /Methods/ are functions that belong to objects
1. An object packages data together with functions that operate on that data. This is a very common organizational strategy in Python.
   #+BEGIN_SRC python
   my_string = 'Hello world!'

   # Call the swapcase method on the my_string object
   print(my_string.swapcase())
   #+END_SRC

2. You can chain methods into processing pipelines
   #+BEGIN_SRC python
   print(my_string.isupper())          # Check whether all letters are uppercase
   print(my_string.upper())            # Capitalize all the letters
   #+END_SRC

   #+BEGIN_SRC python
   # The output of upper() is as string; you can use more string methods on it
   my_string.upper().isupper()
   #+END_SRC

3. (Optional) Strings are immutable. This will be covered later, but may come up here. If it comes up here, this is a good example:
   #+BEGIN_SRC python
   print(my_string.upper())
   print(my_string)
   upper_string = my_string.upper()
   print(upper_string)
   #+END_SRC

4. You can view an object's attributes (i.e. methods and fields) using ~help()~ or ~dir()~. Some attributes are "private"; you're not supposed to use these directly.
   #+BEGIN_SRC python
   # More verbose help
   help(str)
   #+END_SRC

   #+BEGIN_SRC python
   # The short, short version
   dir(my_string)
   #+END_SRC

5. The built-in string methods can be very useful for cleaning up data
   #+BEGIN_SRC python
   bad_string_1 = "  Hello world!   "
   bad_string_2 = "|...goodbye cruel world|"

   print(bad_string_1.strip(),
         bad_string_2.strip("|"))
   #+END_SRC

** (Optional) Python produces informative error messages
1. Python reports a syntax error when it can’t understand the source of a program
   #+BEGIN_SRC python
   name = 'Bob
   age = = 54
   print("Hello world"
   #+END_SRC

2. Python reports a runtime error when something goes wrong while a program is executing

** *(Optional) Beginner Challenge*: What happens when?
Explain in simple terms the order of operations in the following program: when does the addition happen, when does the subtraction happen, when is each function called, etc. What is the final value of radiance?

#+BEGIN_SRC python
radiance = 1.0
radiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5))
#+END_SRC

* Libraries
** Most of the power of a programming language is in its libraries
https://docs.python.org/3/library/index.html

** A program must ~import~ a library module before using it
#+BEGIN_SRC python
import math

print(math.pi)
print(math.cos(math.pi))
#+END_SRC
- Refer to things from the module as ~module-name.thing-name~
- Python uses "." to mean "part of" or "belongs to".

** Use ~help()~ to learn about the contents of a library module
#+BEGIN_SRC python
help(math)                      # user friendly
#+END_SRC

#+BEGIN_SRC python
dir(math)                       # brief reminder, not user friendly
#+END_SRC

** (Optional) Import shortcuts
1. Import specific items from a library module. You want to be careful with this. It's safer to keep the namespace.
   #+BEGIN_SRC python
   from math import cos, pi

   cos(pi)
   #+END_SRC

2. Create an alias for a library module when importing it
   #+BEGIN_SRC python
   import math as m

   print(m.cos(m.pi))
   #+END_SRC

** Python has opinions about how to write your programs
#+BEGIN_SRC python
import this
#+END_SRC

* Lists
Lists are the central data structure in Python; we will explain many things by making analogies to lists.
** A list stores many values in a single structure
#+BEGIN_SRC python
pressure = [0.17, 0.23, 0.54, 0.38, 0.76, 0.43]
print(pressure)
print(len(pressure))
#+END_SRC

** Lists are indexed by position, counting from 0
#+BEGIN_SRC python
print("First item:", pressure[0])
print("Fifth item:" , pressure[4])
#+END_SRC

** You can get a subset of the list by slicing it
1. You slice a list from the start position up to, but not including, the stop position
   #+BEGIN_SRC python
   print(pressure[0:3])
   print(pressure[2:5])
   #+END_SRC

2. You can omit the start position if you're starting at the beginning...
   #+BEGIN_SRC python
   print("First 5 items:", pressure[0:5])
   print("First 5 items, but shorter:", pressure[:5])
   #+END_SRC

3. ...and you /must/ omit the end position if you're going to the end (otherwise it's up to, but not including, the end!)
   #+BEGIN_SRC python
   # This is useful if you don't know how long the list is
   print("Everything but the first 3 items:", pressure[3:])
   #+END_SRC

4. You can add an optional step interval (every 2nd item, every 3rd item, etc.)
   #+BEGIN_SRC python
   print("First 5 items, every other item:", pressure[0:5:2])
   print("Every third item:", pressure[::3])
   #+END_SRC

** Why are lists indexed from 0?
cf. https://stackoverflow.com/a/11364711
1. Slice endpoints are compliments
   In both cases, the number you see represents what you want to do.
   #+BEGIN_SRC python
   # Get the first two items
   print(pressure[:2])

   # Get everything except the first two items
   print(pressure[2:])
   #+END_SRC

2. For non-negative indices, the length of a slice is the difference of the indices
   #+BEGIN_SRC python
   len(pressure[1:3]) == 2
   #+END_SRC

** Some other properties of indexes
1. You can count backwards from the end with negative integers
   #+BEGIN_SRC python
    print("Last item:", pressure[-1])
   #+END_SRC

2. Indexing beyond the end of the collection is an error
   #+BEGIN_SRC python
   pressure[20]
   #+END_SRC

** Lists are mutable
1. You can replace a value at a specific index location
   #+BEGIN_SRC python
   pressure[0] = 0.999
   print(pressure)
   #+END_SRC

2. Add an item to list with ~append()~. This is a /method/ of the list.
   #+BEGIN_SRC python
   primes = [2, 3, 5]
   print(primes)
   primes.append(7)
   print(primes)
   #+END_SRC

3. Add the items from one list to another with ~extend()~
   #+BEGIN_SRC python
   teen_primes = [11, 13, 17, 19]

   # Add all of the elements of teen_primes to primes
   primes.extend(teen_primes)
   print(primes)
   #+END_SRC

4. (Optional) Slice endpoints are compliments, take 2
   #+BEGIN_SRC python
   new_pressure = pressure[:2]
   new_pressure.extend(pressure[2:])

   print(new_pressure == pressure)
   #+END_SRC

** Many functions take collections as arguments
#+BEGIN_SRC python
mean_p = sum(pressure)/len(pressure)
print(mean_p)
#+END_SRC

** (Optional) Removing items from a list
1. Use ~del~ to remove an item at an index location
   #+BEGIN_SRC python
   primes = [2, 3, 5, 7, 9]
   print(primes)
   del primes[4]
   print(primes)
   #+END_SRC

2. Use ~pop()~ to remove the last item and assign it to a variable. This is useful for destructive iteration.
   #+BEGIN_SRC python
   p = primes.pop()

   print('Last prime in list', p)
   print(primes)
   #+END_SRC

** Lists can contain anything
1. You can mix data types
   #+BEGIN_SRC python
   ages = ['Derek', 42, 'Bill', 24, 'Susan', 37]

   # Get first pair
   print(ages[0:2])

   # Get all the names
   print(ages[::2])

   # Get all the ages
   print(ages[1::2])
   #+END_SRC

2. You can put lists inside other lists
   #+BEGIN_SRC python
   ages.append(primes)

   # List in our list
   print(ages)

   # The last item is a list
   print(ages[-1])

   # Get an item from that list
   print(ages[-1][2])
   #+END_SRC

* Strings are (kind of) like lists
** Strings are indexed like lists
1. Use an index to get a single character from a string
   #+BEGIN_SRC python
   element = 'carbon'
   element[0]
   #+END_SRC

2. Use a slice to get a substring
   #+BEGIN_SRC python
   element[0:3]
   #+END_SRC

3. Counting backwards
   #+BEGIN_SRC python
   element[-1]
   #+END_SRC

4. Et cetera

** (Optional) Strings have a length
#+BEGIN_SRC python
len('carbon')
#+END_SRC

** But! Strings are immutable
1. Can't change a string in place
   #+BEGIN_SRC python
   element[0] = 'C'
   #+END_SRC

2. String methods create a new string
   #+BEGIN_SRC python
   print(element.capitalize())
   print(element)
   #+END_SRC

   #+BEGIN_SRC python
   carbon_title = element.capitalize()
   print(carbon_title)
   #+END_SRC

** Building strings with ~.join()~
1. Use ~.join()~ to concatenate strings
   #+BEGIN_SRC python
   date_list = ["3", "17", "2007"]
   date = "/".join(date_list)
   print(date)
   #+END_SRC

2. This is going to be useful for building CSV files
   #+BEGIN_SRC python
   date_list = ["3", "17", "2007"]
   date = ",".join(date_list)
   print(date)
   #+END_SRC

** *(Optional) Beginner Challenge*: From Strings to Lists and Back
1. Given this Python code...
   #+BEGIN_SRC python
   print('string to list:', list('tin'))
   print('list to string:', ''.join(['g', 'o', 'l', 'd']))
   #+END_SRC

2. What does ~list('some string')~ do?
3. What does ~'-'.join(['x', 'y', 'z'])~ generate?

** *Challenge*: Locating the right module
You want to select a random character from a string:
#+BEGIN_SRC python
bases = 'ACTTGCTTGAC'
#+END_SRC

1. Which standard library module could help you? https://docs.python.org/3/library/
2. Which function would you select from that module? Are there alternatives?
3. Try to write a program that uses the function.

*** Solutions:
1. You could try the ~random~ module. The string has 11 characters, each having a positional index from 0 to 10. You could use either ~random.randrange~ or ~random.randint~ functions to get a random integer between 0 and 10, and then pick out the character at that position:

   #+BEGIN_SRC python
   from random import randrange

   random_index = randrange(len(bases))
   print(bases[random_index])
   #+END_SRC

   ...or more compactly:

   #+BEGIN_SRC python
   from random import randrange

   print(bases[randrange(len(bases))])
   #+END_SRC

2. Perhaps you found the ~random.sample()~ function. It allows for slightly less typing:
   #+BEGIN_SRC python
   from random import sample

   print(sample(bases, 1)[0])
   #+END_SRC

* Dictionaries
** Dictionaries are sets of key/value pairs. Instead of being indexed by position, they are indexed by key.
#+BEGIN_SRC python
wave_fc = {"Girma": 4,
           "Sheridan": 3,
           "Morgan": 13}

# Returns 4
wave_fc["Girma"]
#+END_SRC

** Update dictionaries by assigning a key/value pair
1. Update a pre-existing key with a new value
   #+BEGIN_SRC python
   wave_fc["Sheridan"] = 1

   print(wave_fc)
   #+END_SRC

2. Add a new key/value pair
   #+BEGIN_SRC python
   wave_fc["Shaw"] = 11
   #+END_SRC

** (Optional) Check whether the dictionary contains an item
1. Does a key already exist?
   #+BEGIN_SRC python
   "Girma" in wave_fc
   #+END_SRC

2. Does a value already exist (you generally don't want to do this; keys are unique but values are not)?
   #+BEGIN_SRC python
   4 in wave_fc.values()
   #+END_SRC

** (Optional) Delete an item using ~del~ or ~pop()~
#+BEGIN_SRC python
print("Original dictionary", wave_fc)
del wave_fc["Morgan"]
print("1st deletion", wave_fc)

girma_num = wave_fc.pop("Girma")
print("2nd deletion", wave_fc)
print("Returned value", girma_num)
#+END_SRC

** Dictionaries are the natural way to store tree-structured data
As with lists, you can put anything in a dictionary.
#+BEGIN_SRC python
location = {'latitude': [37.28306, 'N'],
            'longitude': [-120.50778, 'W']}

print(location['longitude'][0])
#+END_SRC

** *(Optional) Advanced Challenge*: Convert a list to a dictionary
How can you convert our list of names and ages into a dictionary? Hint: You will need to populate the dictionary with a list of keys and a list of values.

#+BEGIN_SRC python
# Starting data
ages = ['Derek', 42, 'Bill', 24, 'Susan', 37]

# Get dictionary help
help({})
#+END_SRC

*** Solution
#+BEGIN_SRC python
ages_dict = dict(zip(ages[::2], ages[1::2]))
#+END_SRC

* (Optional) Other containers
1. Tuples
2. Sets

* *WEEK 2: Data manipulation with Pandas*
* (Optional) Review collections
** Lists and dictionaries
1. Reference item by index/key
2. Insert item by index/key
3. Indices/keys must be unique

** Strings
1. Similar to lists: Reference item by index, have length
2. Immutable, so need to use string *methods*
3. ~'/'.join()~ is a very useful method

* A very brief introduction to NumPy
1. NumPy is the linear algebra library for Python
   #+BEGIN_SRC python
   import numpy as np

   # Create an array of random numbers
   rand = np.random.rand(3, 4)
   print(rand)
   #+END_SRC

2. Arrays are indexed like lists
   #+BEGIN_SRC python
   print(rand[0,0])
   #+END_SRC

3. Arrays are fast but inflexible - the entire array must be of a single type.

* A very brief introduction to Pandas
1. Pandas is a library for working with spreadsheet-like data ("DataFrames")
2. A DataFrame is a collection (dict) of Series columns
3. Each Series is a 1-dimensional NumPy array with optional row labels (dict-like, similar to R vectors)
4. Therefore, each series inherits many of the abilities (linear algebra) and limitations (single data type) of NumPy

* (Optional) Where are we?
** Python provides functions for working with the file system.
#+BEGIN_SRC python
import os

# print current directory
print("Current working directory:", os.getcwd())
# print all of the files and directories
print("Working directory contents:", os.listdir())
#+END_SRC

** These provide a rich Python alternative to shell functions
#+BEGIN_SRC python
# Get 1 level of subdirectories
print("Just print the sub-directories:", sorted(next(os.walk('.'))[1]))

# Move down one directory
os.chdir("data")
print(os.getcwd())

# Move up one directory
os.chdir("..")
print(os.getcwd())
#+END_SRC

* Reading tabular data into data frames
** Import tabular data using the Pandas library
#+BEGIN_SRC python
import pandas as pd

data = pd.read_csv('data/gapminder_gdp_oceania.csv')
print(data)
#+END_SRC

#+BEGIN_SRC python
# Jupyter Lab will give you nice formatting if you echo
data
#+END_SRC
- File and directory names are strings
- You can use relative or absolute file paths

** Use ~index_col~ to use a column’s values as row indices
Rows are indexed by number by default (0, 1, 2,....). For convenience, we want to index by country:
#+BEGIN_SRC python
data = pd.read_csv('data/gapminder_gdp_oceania.csv', index_col='country')
print(data)
#+END_SRC
- By default, rows are indexed by position, like lists.
- Setting the ~index_col~ parameter lets us index rows by label, like dictionaries. For this to work, the index column needs to have unique values for every row.
- You can verify the contents of the CSV by double-clicking on the file in Jupyter Lab

** Pandas help files are dense; you should prefer the online documentation
1. Main documentation link: https://pandas.pydata.org/docs/user_guide/index.html
2. Pandas can read many different data formats: https://pandas.pydata.org/docs/user_guide/io.html

* Data frames are objects that can tell you about their contents
** Data frames have methods (i.e. functions) that perform operations using the data frame's contents as input
1. Use ~.info()~ to find out more about a data frame
   #+BEGIN_SRC python
   data.info()
   #+END_SRC

2. Use ~.describe()~ to get summary statistics about data
   #+BEGIN_SRC python
   data.describe()
   #+END_SRC

3. (Optional) Look at the first few rows
   #+BEGIN_SRC python
   data.head(1)
   #+END_SRC

** Data frames have fields (i.e. variables) that hold additional information
A "field" is a variable that belongs to an object.
1. The ~.index~ field stores the row Index (list of row labels)
   #+BEGIN_SRC python
   print(data.index)
   #+END_SRC

2. The ~.columns~ field stores the column Index (list of column labels)
   #+BEGIN_SRC python
   print(data.columns)
   #+END_SRC

3. The ~.shape~ variable stores the matrix shape
   #+BEGIN_SRC python
   print(data.shape)
   #+END_SRC

4. Use ~DataFrame.T~ to transpose a DataFrame. This doesn't copy or modify the data, it just changes the caller's view of it.
   #+BEGIN_SRC python
   print(data.T)
   print(data.T.shape)
   #+END_SRC

** (Optional) Pandas introduces some new types
#+BEGIN_SRC python
# DataFrame type
type(data)
type(data.T)

# Series type
type(data['gdpPercap_1952'])

# Index type
type(data.columns)
#+END_SRC
- You can convert data between NumPy arrays, Series, and DataFrames
- You can read data into any of the data structures from files or from standard Python containers

** *(Optional) Beginner Challenge*
1. Read the data in ~gapminder_gdp_americas.csv~ into a variable called ~americas~ and display its summary statistics.
2. After reading the data for the Americas, use ~help(americas.head)~ and ~help(americas.tail)~ to find out what ~DataFrame.head~ and ~DataFrame.tail~ do.
   1. How can you display the first three rows of this data?
   2. How can you display the last three columns of this data? (Hint: You may need to change your view of the data).
3. As well as the ~read_csv~ function for reading data from a file, Pandas provides a ~to_csv~ function to write DataFrames to files. Applying what you’ve learned about reading from files, write one of your DataFrames to a file called ~processed.csv~. You can use ~help~ to get information on how to use ~to_csv~.

*** Solution
#+BEGIN_SRC python
americas = pd.read_csv('data/gapminder_gdp_americas.csv', index_col='country')
americas.describe()
americas.head(3)
americas.T.tail(3)
americas.to_csv('processed.csv')
#+END_SRC

* Subsetting Data
** Treat the data frame as a matrix and select values by position
Use ~DataFrame.iloc[..., ...]~ to select values by their (entry) position. The ~i~ in ~iloc~ stands for "index".
#+BEGIN_SRC python
import pandas as pd
data = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')

data.iloc[0,0]
#+END_SRC

** Treat the data frame as a table and select values by label
This is most common way to get data
1. Use ~DataFrame.loc[..., ...]~ to select values by their label
   #+BEGIN_SRC python
   # This returns a value
   data.loc["Albania", "gdpPercap_1952"]
   #+END_SRC

** Shorten the column names using vectorized string methods
#+BEGIN_SRC python
print(data.columns)

# The columns index can update all of its values in a single operation
data.columns = data.columns.str.strip("gdpPercap_")
print(data.columns)
#+END_SRC

** Use list slicing notation to get subsets of the data frame
1. Select multiple columns or rows using ~.loc~ and a named slice. This generalizes the concept of a slice to include labeled indexes.
   #+BEGIN_SRC python
   # This returns a DataFrame
   data.loc['Italy':'Poland', '1962':'1972']
   #+END_SRC

2. Use ~:~ on its own to mean all columns or all rows. This is Python’s usual slicing notation, which allows you to treat data frames as multi-dimensional lists.
   #+BEGIN_SRC python
   # This returns a DataFrame
   data.loc['Italy':'Poland', :]
   #+END_SRC

3. (Optional) If you want specific rows or columns, pass in a list
   #+BEGIN_SRC python
   data.loc[['Italy','Poland'], :]
   #+END_SRC

4. ~.iloc~ follows list index conventions ("up to, but not including)", but ~.loc~ does the intuitive right thing ("A through B")
      #+BEGIN_SRC python
      index_subset = data.iloc[0:2, 0:2]
      label_subset = data.loc["Albania":"Belgium", "1952":"1962"]

      print(index_subset)
      print(label_subset)
      #+END_SRC

5. Result of slicing can be used in further operations
   #+BEGIN_SRC python
   subset = data.loc['Italy':'Poland', '1962':'1972']

   print(subset.describe())
   print(subset.max())
   #+END_SRC

** (Optional) Treat the data frame as an object and select values using flexible methods
Pandas always drills down to the most parsimonious representation. On one hand, this is convenient; on the other, it violates the Pythonic expectation for strong types.

| Shape of data selection | Pandas return type |
|-------------------------+--------------------|
|                      2D | DataFrame          |
|                      1D | Series             |
|                      0D | single value       |

1. ~.filter()~ always returns the same type as the original item, whereas ~.loc~ and ~.iloc~ might return a data frame or a series.
   #+BEGIN_SRC python
   italy = data.filter(items=["Italy"], axis="index")
   print(italy)
   print(type(italy))
   #+END_SRC

2. ~.filter()~ is a general-purpose, flexible method
   #+BEGIN_SRC python
   help(data.filter)
   data.filter(like="200", axis="columns")
   data.filter(like="200", axis="columns").filter(items=["Italy"], axis="index")
   #+END_SRC

* Filtering (i.e. masking) data
** Use comparisons to select data based on value
1. Show which data frame elements match a criterion.
   #+BEGIN_SRC python
   # Which GDPs are greater than 10,000?
   subset > 10000
   #+END_SRC

2. Use ~.where()~ method to find elements that match the criterion:
   #+BEGIN_SRC python
   fs = subset.where(subset > 10000)
   print(fs)
   #+END_SRC

   1. ~subset > 10000~ returns a data frame of True/False values
   2. ~subset.where()~ filters its contents based on that True/False data frame
   3. This section is more properly called "Masking Data," because it involves operations for overlaying a data frame's values without changing the data frame's shape. We don't drop anything from the data frame, we just replace it with ~NaN~.

3. (Optional) Use the criterion match to filter the data frame's contents. This uses index notation:
   #+BEGIN_SRC python
   subset[subset > 10000]
   #+END_SRC

** You can filter using any method that returns a data frame
#+BEGIN_SRC python
# GDP for all countries greater than the median
subset.where(subset > subset.median())
#+END_SRC

** Use method chaining to create final output without creating intermediate variables
#+BEGIN_SRC python
# The .rank() method turns numerical scores into ranks
subset.rank()
#+END_SRC

#+BEGIN_SRC python
# GDP ranking for all countries greater than the median
subset.where(subset > subset.median()).rank()
#+END_SRC

** Methods we're not going to cover
~.query()~ is a flexible, general-purpose way of filtering data frames.

* Working with missing data
** By default, most numerical operations ignore missing data
Examples include min, max, mean, std, etc.
1. Missing values ignored by default
   #+BEGIN_SRC python
   print("Column means")
   print(fs.mean())

   print("Row means")
   print(fs.mean(axis=1))
   #+END_SRC

2. Force inclusions with the ~skipna~ argument
   #+BEGIN_SRC python
   print("Column means")
   print(fs.mean(skipna=False))

   print("Row means")
   print(fs.mean(axis=1, skipna=False))
   #+END_SRC

** Check for missing values
1. Show which items are missing. "NA" includes ~NaN~ and ~None~. It doesn't include empty strings or ~numpy.inf~.
   #+BEGIN_SRC python
   # Show which items are NA
   fs.isna()
   #+END_SRC

2. Count missing values
   #+BEGIN_SRC python
   # Missing by row
   print(fs.isna().sum())

   # Missing by column
   print(fs.isna().sum(axis=1))

   # Aggregate sum
   fs.isna().sum().sum()
   #+END_SRC

3. Are any values missing?
   #+BEGIN_SRC python
   fs.isna().any(axis=None)
   #+END_SRC

4. (Optional) Are all of the values missing?
   #+BEGIN_SRC python
   fs.isna().all(axis=None)
   #+END_SRC

** Replace missing values
1. Replace with a fixed value
   #+BEGIN_SRC python
   fs_fixed = fs.fillna(99)
   print(fs_fixed)
   #+END_SRC

2. (Optional) Impute missing values. Read the docs, this may or may not be sufficient for your needs.
   #+BEGIN_SRC python
   fs_imputed = fs.interpolate()
   #+END_SRC

** Drop missing values
Drop all rows with missing values
#+BEGIN_SRC python
fs_drop = fs.dropna()
#+END_SRC

** *Challenge: Filter and trim with a boolean vector*
A DataFrame is a dictionary of Series columns. With this in mind, experiment with the following code and try to explain what each line is doing. What operation is it performing, and what is being returned?

Feel free to use ~print()~, ~help()~, ~type()~, etc as you investigate.

#+BEGIN_SRC python
fs["1962"]
fs["1962"].notna()
fs[fs["1962"].notna()]
#+END_SRC

*** Solution
1. Line 1 returns the column as a Series vector
2. Line 2 returns a boolean Series vector (True/False)
3. Line 3 performs /boolean indexing/ on the DataFrame using the Series vector. It only returns the rows that are True (i.e. it performs true filtering).

* Sorting and grouping
** Motivating example: Calculate the wealth Z-score for each country
#+BEGIN_SRC python
# Calculate z scores for all elements
z = (data - data.mean())/data.std()

# Get the mean z score for each country (i.e. across all columns)
mean_z = z.mean(axis=1)

# Group countries into "wealthy" (z > 0) and "not wealthy" (z <= 0)
z_bool = mean_z > 0

print(mean_z)
print(z_bool)
#+END_SRC

** Append new columns to the data frame containing our summary statistics
Data frames are dictionaries of Series:
#+BEGIN_SRC python
data["mean_z"] = mean_z
data["wealthy"] = z_bool
#+END_SRC

** Sort and group by new columns
#+BEGIN_SRC python
data.sort_values(by="mean_z")
#+END_SRC

#+BEGIN_SRC python
# Get descriptive statistics for the group
data.groupby("wealthy").mean()
data.groupby("wealthy").describe()
#+END_SRC

* Write output
Capture the results of your filter in a new file, rather than overwriting your original data.
#+BEGIN_SRC python
# Save to a new CSV, preserving your original data
data.to_csv('gapminder_gdp_europe_normed.csv')

# If you don't want to preserve row names:
#data.to_csv('gapminder_gdp_europe_normed.csv', index=False)
#+END_SRC

* Working with multiple tables (in an SQL-like manner)
** Concatenating data frames
#+BEGIN_SRC python
surveys = pd.read_csv('data/surveys.csv', index_col="record_id")
print(surveys.shape)
#+END_SRC

#+BEGIN_SRC python
df1 = surveys.head(10)
df2 = surveys.tail(10)

df3 = pd.concat([df1, df2])
print(df3.shape)
#+END_SRC

** Joining data frames
1. Import species data
   #+BEGIN_SRC python
   species = pd.read_csv('data/species.csv', index_col="species_id")
   print(species.shape)
   #+END_SRC

2. Join tables on common column. The "left" join is a strategy for augmenting the first table (surveys) with information from the second table (species).
   #+BEGIN_SRC python
   df_join = surveys.merge(species, on="species_id", how="left")
   print(df_join.head())
   print(df_join.shape)
   #+END_SRC

3. The resulting table loses its index because ~surveys.record_id~ is not being used in the join. To keep ~record_id~ as the index for the final table, we need to retain it as an explicit column.
   #+BEGIN_SRC python
   # Don't set record_id as index during initial import
   surveys = pd.read_csv('data/surveys.csv')
   df_join = surveys.merge(species, on="species_id", how="left").set_index("record_id")

   df_join.index
   #+END_SRC

4. Get the subset of species that match a criterion, and join on that subset. The "inner" join only includes rows where both tables match on the key column; it's a strategy for filtering the first table by the second table.
   #+BEGIN_SRC python
   # Get the taxa column, masking the rows based on which values match "Bird"
   birds = species[species["taxa"] == "Bird"]
   df_birds = surveys.join(birds, on="species_id").set_index("record_id")

   print(df_birds.head())
   print(df_birds.shape)
   #+END_SRC

* (Optional) Adding rows to DataFrames
A row is a view onto the /nth/ item of each of the column Series. Appending rows is a performance bottleneck because it requires a separate append operation for each Series. You should concatenate data frames instead.s

1. Create a single row as a data frame and concatenate it.
   #+BEGIN_SRC python
   row = pd.DataFrame({"1962": 5000, "1967": 5000, "1972": 5000}, index=["Latveria"])
   pd.concat([subset, row])
   #+END_SRC

2. If you have individual rows as Series, ~pd.concat()~ will produce a data frame.
   #+BEGIN_SRC python
   # Get each row as a Series
   italy = data.loc["Italy", :]
   poland = data.loc["Poland", :]

   # Omitting axis argument (or axis=0) concatenates the 2 series end-to-end
   # axis=1 creates a 2D data frame
   # Transpose recovers original orientation
   # Column labels come from Series index
   # Row labels come from Series name
   pd.concat([italy, poland], axis=1).T
   #+END_SRC

* Scientific Computing Libraries
1. SciPy projects
   1. Numpy: Linear algebra
   2. Pandas
   3. Scipy.stats: Probability distributions and basic tests
2. Statsmodels: Statistical models and formulae built on Scipy.stats
3. Scikit-Learn: Machine learning tools built on NumPy
4. Tensorflow/PyTorch: Deep learning and other voodoo

** (Optional) Statsmodels regression example
1. Import data
   #+Begin_SRC python
   dataa = pd.read_csv('surveys.csv')

   # Check for NaN
   print("Valid weights:", data['weight'].count())
   print("NaN weights:", data['weight'].isna().sum())
   print("Valid lengths:", data['hindfoot_length'].count())
   print("NaN lengths:", data['hindfoot_length'].isna().sum())
   #+END_SRC

2. Fit OLS regression model
   #+BEGIN_SRC python
   from statsmodels.formula.api import ols

   model = ols("weight ~ hindfoot_length", data, missing='drop').fit()
   print(model.summary())
   #+END_SRC

3. Generic parameters for all models
   #+BEGIN_SRC python
   importort statsmodels

   help(statsmodels.base.model.Model)
   #+END_SRC

** (Optional) Getting started with machine learning estimators
https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
https://scikit-learn.org/stable/_static/ml_map.png

* (Optional) Things we didn't talk about
1. pipe
2. map/applymap/apply (in general you should prefer vectorized functions)

* (Optional) Pandas method chaining in the wild
https://gist.githubusercontent.com/adiamaan92/d8ebee8937d271452def2a7314993b2f/raw/ce9fbb5013d94accf0779a25e182c4be77678bd0/wine_mc_example.py
#+BEGIN_SRC python
wine.rename(columns={"color_intensity": "ci"})
.assign(color_filter=lambda x: np.where((x.hue > 1) & (x.ci > 7), 1, 0))
.query("alcohol > 14 and color_filter == 1")
.sort_values("alcohol", ascending=False)
.reset_index(drop=True)
.loc[:, ["alcohol", "ci", "hue"]]
#+END_SRC

* (Optional) Introspecting on the DataFrame object
1. DataFrames have a huge number of fields and methods, so dir() is not very useful
   #+BEGIN_SRC python
   print(dir(data))
   #+END_SRC

2. Create a new list that filters out internal attributes
   #+BEGIN_SRC python
   df_joinpublic = [item for item in dir(data) if not item.startswith('_')]
   print(df_public)
   #+END_SRC

3. (Optional) Pretty-print the new list
   #+BEGIN_SRC python
   importort pprint

   pp = pprint.PrettyPrinter(width=100, compact=True, indent=2)
   pp.pprint(df_public)
   #+END_SRC

4. Objects have fields (i.e. data/variables) and methods (i.e. functions/procedures). The difference between a method and a function is that methods are attached to objects, whereas functions are free-floating ("first-class citizens"). Methods and functions are "callable":
   #+BEGIN_SRC python
   # GeneratorExitenerate a list of public methods and a list of public fields. We do this
   # by testing each attribute to determine whether it is "callable".
   # NB: Because Python allows you to override any attribute at runtime,
   # testing with `callable` is not always reliable.

   # List of methods (callable attributes)
   df_methods = [item for item in dir(data) if not item.startswith('_')
                 and callable(getattr(data, item))]
   # List of fields (non-callable attributes)
   df_attr = [item for item in dir(data) if not item.startswith('_')
              and not callable(getattr(data, item))]

   pp.pprint(df_methods)
   pp.pprint(df_attr)
   #+END_SRC

* (Carpentries version) Group By: split-apply-combine
1. Split data according to criterion, do numeric transformations, then recombine.
   #+BEGIN_SRC python
   # Get all GDPs greater than the mean
   mask_higher = data > data.mean()

   # Count the number of time periods in which each country exceeds the mean
   higher_count = mask_higher.aggregate('sum', axis=1)

   # Create a normalized wealth-over-time score
   wealth_score = higher_count / len(data.columns)
   wealth_score
   #+END_SRC

2. A DataFrame is a spreadsheet, but it is also a dictionary of columns.
   #+BEGIN_SRC python
   data['gdpPercap_1962']
   #+END_SRC

3. Add column to data frame
   #+BEGIN_SRC python
   # Warningealth Score is a series
   type(wealth_score)

   data['normalized_wealth'] = wealth_score
   #+END_SRC

* *WEEK 3: Building Programs*
* Notebooks vs Python scripts
** Differences between .ipynb and .py
1. Export notebook to .py file
2. Move .py file into data directory
3. Compare files in TextEdit/Notepad

** Workflow differences between notebooks and scripts
Broadly, a trade-off between managing big code bases and making it easy to experiment. See: https://github.com/elliewix/Ways-Of-Installing-Python/blob/master/ways-of-installing.md#why-do-you-need-a-specific-tool
1. Interactive testing and debugging
2. Graphics integration
3. Version control
4. Remote scripts

* Python from the terminal
1. Python is an interactive interpreter (REPL)
   #+BEGIN_SRC bash
   python
   #+END_SRC

2. Python is a command line program
   #+BEGIN_SRC python
   # hello.py
   print("Hello!")
   #+END_SRC

   #+BEGIN_SRC bash
   python hello.py
   #+END_SRC

3. (Optional) Python programs can accept command line arguments as inputs
   1. List of command line inputs: ~sys.argv~ (https://docs.python.org/3/library/sys.html#sys.argv)
   2. Utility for working with arguments: ~argparse~ (https://docs.python.org/3/library/argparse.html)

* For Loops
** A ~for~ loop executes commands once for each value in a collection
"For each thing in this group, do these operations"
#+BEGIN_SRC python
for number in [2, 3, 5]:
    print(number)
#+END_SRC

- A for loop is made up of a collection, a loop variable, and a body
- The collection, *[2, 3, 5]*, is what the loop is being run on.
- The body, *print(number)*, specifies what to do for each value in the collection.
- The loop variable, *number*, is what changes for each iteration of the loop (i.e. the “current thing”)

** The first line of the ~for~ loop must end with a colon, and the body must be indented
Whitespace is syntactically meaningful!

#+BEGIN_SRC python
for number in [2, 3, 5]:
print(number)
#+END_SRC

#+BEGIN_SRC python
firstName = "Jon"
lastName = "Smith"
#+END_SRC

** Loop variables can be called anything
#+BEGIN_SRC python
for bob in [2, 3, 5]:
    print(bob)
#+END_SRC

** The body of a loop can contain many statements
#+BEGIN_SRC python
primes = [2, 3, 5]
for p in primes:
    squared = p ** 2
    cubed = p ** 3
    print(p, squared, cubed)
#+END_SRC

** (Optional) Use ~range()~ to iterate over a sequence of numbers
#+BEGIN_SRC python
for number in range(0, 3):
    print(number)
#+END_SRC

- range() produces numbers on demand (a "generator" function)
- useful for tracking progress

** (Optional) Use ~enumerate()~ to iterate over a sequence of items and their positions
#+BEGIN_SRC python
for number, p in enumerate(primes):
    print(number, ":", p)
#+END_SRC

** Common pattern 1: Accumulate a running total
Initialize an accumulator variable to zero, the empty string, or the empty list; then iteratively update the variable with values from a collection.
#+BEGIN_SRC python
total = 0
for number in range(7):
   total = total + number
print(total)
#+END_SRC

** Common pattern 2: Create a new collection from an existing collection
#+BEGIN_SRC python
prime_exponents = []
for p in primes:
   prime_exponents.append(p**2)

print(prime_exponents)
#+END_SRC

** (Optional) Dictionary iteration
1. Iterate over key: value pairs
   #+BEGIN_SRC python
   ages = {'Derek': 42,
           'Bill': 24,
           'Susan': 37}

   for key, val in ages.items():
       print(key, val)
   #+END_SRC

2. You can iterate over keys and values separately
   #+BEGIN_SRC python
   # Iterate over keys; you can also explicitly call .keys()
   for key in ages:
       print(key)

   # Iterate over values
   for val in ages.values():
       print(val)
   #+END_SRC

3. Iteration can be useful for unpacking complex dictionaries
   #+BEGIN_SRC python
   localsation = {'latitude': [37.28306, 'N'],
               'longitude': [-120.50778, 'W']}

   for key, val in location.items():
       print(key, 'is', val[0], val[1])
   #+END_SRC

** (Optional) How do you know if an object is iterable?
1. Lists, dictionaries, and strings are iterable
   #+BEGIN_SRC python
   hasattr(location, "__iter__")
   #+END_SRC

2. Integers are not iterable
   #+BEGIN_SRC python
   hasattr(5, "__iter__")
   #+END_SRC

** Don't use ~for~ loops with DataFrames or Numpy matrices
There is almost always a faster vectorized function that does what you want.

* Looping Over Data Sets
** File paths as an example of increasing abstraction in program development
1. File paths as literal strings
2. File paths as string patterns
3. File paths as abstract Path objects

** Use a ~for~ loop to process files given a list of their names
#+BEGIN_SRC python
import pandas as pd

file_list = ['data/gapminder_gdp_africa.csv', 'data/gapminder_gdp_asia.csv']
for filename in file_list:
    data = pd.read_csv(filename, index_col='country')
    print(filename)
    print(data.head(1))
#+END_SRC

** Use glob.glob to find sets of files whose names match a pattern
1. Get a list of all the CSV files
   #+BEGIN_SRC python
   import glob
   glob.glob('data/*.csv')
   #+END_SRC

2. In Unix, the term “globbing” means “matching a set of files with a pattern”. It uses shell expansion rules, *not* regular expressions, so there's an upper limit to how flexible it can be. The most common patterns are:
   - `*` meaning “match zero or more characters”
   - `?` meaning “match exactly one character”

3. Get a list of all the Gapminder CSV files
   #+BEGIN_SRC python
   glob.glob('data/gapminder_*.csv')
   #+END_SRC

4. Exclude the "all" CSV file
   #+BEGIN_SRC python
   glob.glob('data/gapminder_[!all]*.csv')
   #+END_SRC

** Use glob and a ~for~ loop to process batches of files
#+BEGIN_SRC python
data_frames = []
for filename in glob.glob('data/gapminder_[!all]*.csv'):
    data = pd.read_csv(filename)
    data_frames.append(data)

all_data = pd.concat(data_frames)
print(all_data.shape)
#+END_SRC

* Conditionals
** Evaluating the truth of a statement
1. Value of a variable
   #+BEGIN_SRC python
   mass = 3

   print(mass == 3)
   print(mass > 5)
   print(mass < 4)
   #+END_SRC

2. Membership in a collection
   #+BEGIN_SRC python
   primes = [2, 3, 5]

   print(2 in primes)
   print(7 in primes)
   #+END_SRC

3. Truth of a collection
   Note that ~any()~ and ~all()~ evaluate each item using ~.__bool__()~ or ~.__len()__~, which tells you whether an item is "truthy" or "falsey" (i.e. interpreted as being true or false).
   #+BEGIN_SRC python
   my_list = [2.75, "green", 0]

   print(any(my_list))
   print(all(my_list))
   #+END_SRC

4. (Optional) Understanding "truthy" and "falsey" values in Python (cf. https://stackoverflow.com/a/53198991)
   Every value in Python, regardless of type, is interpreted as being ~True~ except for the following values (which are interpreted as ~False~). "Truthy" values satisfy ~if~ or ~while~ statements; "Falsey" values do not.
   1. Constants defined to be false: ~None~ and ~False~.
   2. Zero of any numeric type: ~0~, ~0.0~, ~0j~, ~Decimal(0)~, ~Fraction(0, 1)~
   3. Empty sequences and collections: ~''~, ~()~, ~[]~, ~{}~, ~set()~, ~range(0)~

** Use ~if~ statements to control whether or not a block of code is executed
An ~if~ statement (more properly called a conditional statement) controls whether some block of code is executed or not.

#+BEGIN_SRC python
mass = 3.5
if mass > 3.0:
    print(mass, 'is large')
#+END_SRC

#+BEGIN_SRC python
mass = 2.0
if mass > 3.0:
    print (mass, 'is large')
#+END_SRC

Structure is similar to a ~for~ statement:
  - First line opens with ~if~ and ends with a colon
  - Body containing one or more statements is indented (usually by 4 spaces)

** Use else to execute a block of code when an if condition is not true
~else~ can be used following an ~if~. This allows us to specify an alternative to execute when the if branch isn’t taken.
#+BEGIN_SRC python
if m > 3.0:
    print(m, 'is large')
else:
    print(m, 'is small')
#+END_SRC

** Use ~elif~ to specify additional tests
May want to provide several alternative choices, each with its own test; use ~elif~ (short for “else if”) and a condition to specify these.
#+BEGIN_SRC python
if m > 9.0:
    print(m, 'is HUGE')
elif m > 3.0:
    print(m, 'is large')
else:
    print(m, 'is small')
#+END_SRC

- Always associated with an ~if~.
- Must come before the ~else~ (which is the “catch all”).

** (Optional) Conditionals are often used inside loops
Not much point using a conditional when we know the value (as above), but useful when we have a collection to process.
#+BEGIN_SRC python
masses = [3.54, 2.07, 9.22, 1.86, 1.71]
for m in masses:
    if m > 9.0:
        print(m, 'is HUGE')
    elif m > 3.0:
        print(m, 'is large')
    else:
        print(m, 'is small')
#+END_SRC

** (Optional) Conditions are tested once, in order
Python steps through the branches of the conditional in order, testing each in turn. Order matters! The following is wrong:
#+BEGIN_SRC python
grade = 85
if grade >= 70:
    print('grade is C')
elif grade >= 80:
    print('grade is B')
elif grade >= 90:
    print('grade is A')
#+END_SRC

** (Optional) Compound Relations Using ~and~, ~or~, and Parentheses
Often, you want some combination of things to be true. You can combine relations within a conditional using ~and~ and ~or~. Continuing the example above, suppose you have:
#+BEGIN_SRC python
mass     = [ 3.54,  2.07,  9.22,  1.86,  1.71]
velocity = [10.00, 20.00, 30.00, 25.00, 20.00]

i = 0
for i in range(5):
    if mass[i] > 5 and velocity[i] > 20:
        print("Fast heavy object.  Duck!")
    elif mass[i] > 2 and mass[i] <= 5 and velocity[i] <= 20:
        print("Normal traffic")
    elif mass[i] <= 2 and velocity[i] <= 20:
        print("Slow light object.  Ignore it")
    else:
        print("Whoa!  Something is up with the data.  Check it")
#+END_SRC
- Use () to group subsets of conditions
- Aside: For a more natural way of working with many lists, look at ~zip()~

** (Optional) Use pathlib to write code that works across operating systems
1. Pathlib provides cross-platform path objects
   #+BEGIN_SRC python
   from pathlib import Path

   relative_path = Path("data")   # data subdirectory
   # relative_path = Path()       # current directory
   print("Relative path:", relative_path)
   print("Absolute path:", relative_path.absolute())
   #+END_SRC

2. The file objects have methods that provide much better information about files and directories.
   #+BEGIN_SRC python
   #Note the careful testing at each level of the code.
   if relative_path.exists():
       for filename in relative_path.glob('gapminder_*.csv'):
           if filename.is_file():
               data = pd.read_csv(filename)
               print(filename)
               print(data.head(1))
   #+END_SRC

* Generic file handling
Pandas understands specific file types, but what if you need to work with a generic file?
** Open the file with a context manager
#+BEGIN_SRC python
with open("data/bouldercreek_09_2013.txt", "r") as infile:
    lines = infile.readlines()
#+END_SRC
- The context manager closes the file when you're done reading it
- ~"bouldercreek_09_2013.txt"~ is the name of the file
- ~infile~ is a variable that refers to the file on disk

** A file is a collection of lines
~.readlines()~ produces the file contents as a list of lines; each line is a string.
#+BEGIN_SRC python
print(len(text))
print(type(text))

# View the first 10 lines
print(text[:10])
#+END_SRC

** Strings contain formatting marks
Compare the following:
#+BEGIN_SRC python
# This displays the nicely-formatted document
print(lines[0])
#+END_SRC

#+BEGIN_SRC python
# This shows the true nature of the string; you can see newlines (/n),
# tabs (/t), and other hidden characters
lines[0]
#+END_SRC

* Text processing
** Use string methods to determine which lines to keep
1. The file contains front matter that we can discard
   #+BEGIN_SRC python
   tabular_lines = []
   for line in lines:
       if not line.startswith("#"):
           tabular_lines.append(line)
   #+END_SRC

2. Now the first line is tab-separated data. Note that the print statement /prints/ the tabs instead of showing us the ~\t~ character.
   #+BEGIN_SRC python
   tabular_lines[0]
   #+END_SRC

** Open an output file for writing
#+BEGIN_SRC python
outfile_name = "data/tabular_data.txt"

with open(outfile_name, "w") as outfile:
    outfile.writelines(tabular_lines)
#+END_SRC

** Format output as a comma-delimited text file
1. Strip trailing whitespace
   #+BEGIN_SRC python
   stripped_line = tabular_lines[0].strip()
   stripped_line
   #+END_SRC

2. Split each line into a list based using the tabs.
   #+BEGIN_SRC python
   split_line = stripped_line.split("\t")
   split_line
   #+END_SRC

3. Use a special-purpose library to create a correctly-formatted CSV file
   #+BEGIN_SRC python
   import csv

   outfile_name = "data/csv_data.csv"
   with open(outfile_name, "w") as outfile:
       writer = csv.writer(outfile)
       for line in tabular_lines:
           csv_line = line.strip().split("\t")
           writer.writerow(csv_line)
   #+END_SRC

4. You can initialize ~csv.reader~ and ~csv.writer~ with different "dialects" or with custom delimiters and quotechars; see https://docs.python.org/3/library/csv.html

** (Optional) Avoid memory limitations by processing the input file one line at a time
#+BEGIN_SRC python
infile_name = "data/bouldercreek_09_2013.txt"
outfile_name = "data/csv_data.csv"

with open(infile_name, "r") as infile, open(outfile_name, "w") as outfile:
    writer = csv.writer(outfile)
    for line in infile:
        if not line.startswith("#"):
            writer.writerow(line.strip().split("\t"))
#+END_SRC

** (Optional) Notes
1. Pandas has utilities for reading fixed-width files: https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html
2. Saving datasets with new-style string formatting
   #+BEGIN_SRC python
   for i in datasets_list:
      do_something(f'{i}.png'
   #+END_SRC

* Writing Functions
** Break programs down into functions to make them easier to understand
- Human beings can only keep a few items in working memory at a time.
- Understand larger/more complicated ideas by understanding and combining pieces
- Functions serve the same purpose in programs:
  1. Encapsulate complexity so that we can treat it as a single “thing”
  2. Removes complexity from remaining code, making it easier to test
  3. Enables re-use: Write one time, use many times

** Define a function using ~def~ with a name, parameters, and a block of code
#+BEGIN_SRC python
def print_greeting():
    print('Hello!')
#+END_SRC

- Begin the definition of a new function with ~def~, followed by the name of the function.
- Must obey the same rules as variable names.
- Parameters in parentheses; empty parentheses if the function doesn’t take any inputs.
- Indent function body

** Defining a function does not run it
#+BEGIN_SRC python
print_greeting()
#+END_SRC

- Like assigning a value to a variable
- Must call the function to execute the code it contains.

** Arguments in call are matched to parameters in definition
1. Positional arguments
   #+BEGIN_SRC python
   def print_date(year, month, day):
       joined = '/'.join([year, month, day])
       print(joined)

   print_date(1871, 3, 19)
   #+END_SRC

2. (Optional) Keyword arguments
   #+BEGIN_SRC python
   print_date(month=3, day=19, year=1871)
   #+END_SRC

** Functions may return a result to their caller using ~return~
1. Use ~return ...~ to give a value back to the caller. ~return~ ends the function's execution and /returns/ you to the code that originally called the function.
   #+BEGIN_SRC python
   def average(values):
       """Return average of values, or None if no values are supplied."""

       if len(values) == 0:
           return None
       else:
           return sum(values) / len(values)
   #+END_SRC

   #+BEGIN_SRC python
   a = average([1, 3, 4])
   print(a)
   #+END_SRC

2. You should explicitly handle common problems:
   #+BEGIN_SRC python
   print(average([]))
   #+END_SRC

3. Notes:
   1. ~return~ can occur anywhere in the function, but functions are easier to understand if return occurs:
      1. At the start to handle special cases
      2. At the very end, with a final result
   2. Docstring provides function help. Use triple quotes if you need the docstring to span multiple lines.

** *Challenge (option 1): Encapsulate text processing in a function*
Write a function that takes ~line~ as an input and returns the information required by ~writer.writerow()~.

** *Challenge (option 2): Encapsulate data processing in a function*
Write a function that encapsulates the data normalization from the Pandas workshop into a function. The function should:
1. Take a data frame as its input
2. Calculate the mean Z score for each country
3. Divide countries into "wealthy" and "non-wealthy" categories
4. Add this information to the data frame as new columns
5. Return the modified data frame

*** Solution
#+BEGIN_SRC python
import pandas as pd
import glob

def norm_data(data):
    """Add a Z score column to each data set."""

    # Calculate z scores for all elements
    z = (data - data.mean())/data.std()

    # Get the mean z score for each country
    mean_z = z.mean(axis=1)

    # Group countries into "wealthy" (z > 0) and "not wealthy" (z <= 0)
    z_bool = mean_z > 0

    # Append to DataFrame
    data["mean_z"] = mean_z
    data["wealthy"] = z_bool

for filename in glob.glob('data/gapminder_*.csv'):
    # Print a status message
    print("Current file:", filename)

    # Read the data into a DataFrame and modify it
    data = pd.read_csv(filename)
    norm_data(data)

    # Generate an output file name
    parts = filename.split(".csv")
    newfile = ''.join([parts[0], "_normed.csv"])
    data.to_csv(newfile)
#+END_SRC

** (Optional) A worked example: The Lorenz attractor
https://matplotlib.org/stable/gallery/mplot3d/lorenz_attractor.html

* (Carpentries version) Conditionals
** Use ~if~ statements to control whether or not a block of code is executed
An ~if~ statement (more properly called a conditional statement) controls whether some block of code is executed or not.

#+BEGIN_SRC python
mass = 3.54
if mass > 3.0:
    print(mass, 'is large')

mass = 2.07
if mass > 3.0:
    print (mass, 'is large')
#+END_SRC
Structure is similar to a ~for~ statement:
  - First line opens with ~if~ and ends with a colon
  - Body containing one or more statements is indented (usually by 4 spaces)

** Conditionals are often used inside loops
Not much point using a conditional when we know the value (as above), but useful when we have a collection to process.
#+BEGIN_SRC python
masses = [3.54, 2.07, 9.22, 1.86, 1.71]
for m in masses:
    if m > 3.0:
        print(m, 'is large')
#+END_SRC

** Use else to execute a block of code when an if condition is not true
~else~ can be used following an ~if~. This allows us to specify an alternative to execute when the if branch isn’t taken.
#+BEGIN_SRC python
masses = [3.54, 2.07, 9.22, 1.86, 1.71]
for m in masses:
    if m > 3.0:
        print(m, 'is large')
    else:
        print(m, 'is small')
#+END_SRC

** Use ~elif~ to specify additional tests
May want to provide several alternative choices, each with its own test; use ~elif~ (short for “else if”) and a condition to specify these.
#+BEGIN_SRC python
masses = [3.54, 2.07, 9.22, 1.86, 1.71]
for m in masses:
    if m > 9.0:
        print(m, 'is HUGE')
    elif m > 3.0:
        print(m, 'is large')
    else:
        print(m, 'is small')
#+END_SRC
- Always associated with an ~if~.
- Must come before the ~else~ (which is the “catch all”).

** Conditions are tested once, in order
Python steps through the branches of the conditional in order, testing each in turn. Order matters! The following is wrong:
#+BEGIN_SRC python
grade = 85
if grade >= 70:
    print('grade is C')
elif grade >= 80:
    print('grade is B')
elif grade >= 90:
    print('grade is A')
#+END_SRC

**  Use conditionals in a loop to “evolve” the values of variables
#+BEGIN_SRC python
velocity = 10.0
for i in range(5): # execute the loop 5 times
    print(i, ':', velocity)
    if velocity > 20.0:
        velocity = velocity - 5.0
    else:
        velocity = velocity + 10.0
print('final velocity:', velocity)
#+END_SRC
- This is how dynamical systems simulations work

** Compound Relations Using ~and~, ~or~, and Parentheses (optional)
Often, you want some combination of things to be true. You can combine relations within a conditional using ~and~ and ~or~. Continuing the example above, suppose you have:
#+BEGIN_SRC python
mass     = [ 3.54,  2.07,  9.22,  1.86,  1.71]
velocity = [10.00, 20.00, 30.00, 25.00, 20.00]

i = 0
for i in range(5):
    if mass[i] > 5 and velocity[i] > 20:
        print("Fast heavy object.  Duck!")
    elif mass[i] > 2 and mass[i] <= 5 and velocity[i] <= 20:
        print("Normal traffic")
    elif mass[i] <= 2 and velocity[i] <= 20:
        print("Slow light object.  Ignore it")
    else:
        print("Whoa!  Something is up with the data.  Check it")
#+END_SRC
- Use () to group subsets of conditions
- Aside: For a more natural way of working with many lists, look at ~zip()~

* (Optional) Variable Scope
* (Optional) Programming Style
* *WEEK 4: (future update) Visualization with Matplotlib and Seaborn*
* COMMENT Graphs
Fundamentally, graphs communicate two types of information:
1. Relationships or trends among data
2. The distribution of data

** Big 5 graphs
1. Line plot
2. Scatter plot
3. Bar plot
4. Histogram
5. Box plot

* COMMENT Plotting with Matplotlib
** Create a basic plot
#+BEGIN_SRC python
import matplotlib.pyplot as plt
fig, ax = plt.subplots()

time = [0, 1, 2, 3]
position = [0, 100, 200, 300]

ax.plot(time, postion)

fig
#+END_SRC
** Oceania basic plot
** Two kinds of plotting objects
#+BEGIN_SRC python
print(type(fig))
print(type(ax))
#+END_SRC
- Figure objects handle display, printing, saving, etc.
- Axes objects contain graph information

** Three ways of showing a figure (optional)
1. Show figure inline (Jupyter Lab default)
   #+BEGIN_SRC python
   fig
   #+END_SRC

2. Show figure in a separate window (command line default)
   #+BEGIN_SRC python
   fig.show()
   #+END_SRC

3. Show figure in a separate window from Jupyter Lab. You may need to specify a different "backend" parameter for ~matplotlib.use()~ depending on your exact setup: https://matplotlib.org/stable/tutorials/introductory/usage.html#the-builtin-backends
   #+BEGIN_SRC python
   import matplotlib

   matplotlib.use('TkAgg')

   fig.show()
   #+END_SRC

** Line Plots
1. Create mock data
   #+BEGIN_SRC python
   import numpy as np

   y = np.random.random(10) # outputs an array of 10 random numbers between 0 and 1
   x = np.arange(1980,1990,1) # generates an ordered array of numbers from 1980 to 1989

   # Check that x and y contain the same number of values
   assert len(x) == len(y)

   # Turn y into a percentage
   y = y*100
   #+END_SRC

2. Create the basic plot
   #+BEGIN_SRC python
   fig, ax = plt.subplots()
   ax.plot(x, y)
   #+END_SRC

3. Show available styles (What is the local equivalent of this global command?)
   #+BEGIN_SRC python
   # What are the global styles?
   plt.style.available
   #+END_SRC

   #+BEGIN_SRC python
   # Set a global figure style
   plt.style.use("dark_background")

   # The style is only applied to new figures, not pre-existing figures
   fig
   #+END_SRC

   #+BEGIN_SRC python
   # Re-creating the figure applies the new style
   fig, ax = plt.subplots()
   ax.plot(x, y)
   #+END_SRC

4. Add figure information
   In principle, nearly every element on a Matplotlib figure is independently modifiable.

   #+BEGIN_SRC python
   # modify figure size, axes and fonts
   fig, ax = plt.subplots(figsize=(8,6)) #(width, height) inches
   ax.plot(x, y, color='darkorange', linewidth=2, marker='o')

   # add title and axes label, adjust font size and style

   ax.set_title("Percent Change in Stock X", fontsize=22, fontweight='bold')
   ax.set_xlabel(" Years ", fontsize=20, fontweight='bold')
   ax.set_ylabel(" % change ", fontsize=20, fontweight='bold')

   # adjust tick labels
   ax.tick_params(axis='both', which='major', labelsize=18)

   # add a grid
   ax.grid(True)
   #+END_SRC

5. What is an object?
   Objects encapsulate behaviors
   - Lists, dictionaries, and DataFrames are collections of data
   - Objects are collections of data and functions

6. Matplotlib object syntax
   - The ~object.set_field(value)~ usage is taken from Java, which was popular in 2003 when Matplotlib was developing its object-oriented syntax
   - You get values back out with ~object.get_field(value)~
   - The Pythonic way to set a value would be ~object.field = value~. However, the Matplotlib getters and setters do a lot of internal bookkeeping, so if you try to set field values directly you will get errors. For example, compare ~ax.get_ylabel()~ with ~ax.yaxis.label~.
   - Read "The Lifecycle of a Plot": https://matplotlib.org/stable/tutorials/introductory/lifecycle.html
   - Read "Why you hate Matplotlib": https://ryxcommar.com/2020/04/11/why-you-hate-matplotlib/

7. Save your figure
   #+BEGIN_SRC python
   fig.savefig("mygraph_dark.png", dpi=300, bbox_inches='tight')
   #+END_SRC

** Explore your data with Pandas
1. Import data
   #+BEGIN_SRC python
   import pandas as pd

   data = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')
   #+END_SRC

2. Transform column headers into an ordinal scale
   1. Original column names are object (i.e. string) data
      #+BEGIN_SRC python
      data.columns
      #+END_SRC

   2. Pull out integer portion of strings
      #+BEGIN_SRC python
      years = data.columns.str.strip('gdpPercap_')
      years
      #+END_SRC

   3. Convert the years columns into integer years and replace DataFrame column headers
      #+BEGIN_SRC python
      data.columns = years.astype(int)
      data.columns
      #+END_SRC

3. Plot directly with Pandas
   #+BEGIN_SRC python
   data.loc['Austria'].plot()
   #+END_SRC

** Plot directly from Pandas (optional)
1. The basic plot syntax
   #+BEGIN_SRC python
   ax = data.loc['Austria'].plot()
   fig = ax.get_figure()
   fig
   #+END_SRC

2. Decorate your Pandas plot
   #+BEGIN_SRC python
   ax = data.loc['Austria'].plot(figsize=(8,6), color='darkgreen', linewidth=2, marker='*')
   ax.set_title("GDP of Austria", fontsize=22, fontweight='bold')
   ax.set_xlabel("Years",fontsize=20, fontweight='bold' )
   ax.set_ylabel("GDP",fontsize=20, fontweight='bold' )

   fig = ax.get_figure()
   fig
   #+END_SRC

3. The equivalent Matplotlib plot (optional)
   #+BEGIN_SRC python
   # extract the x and y values from dataframe
   x_years = data.columns
   y_gdp = data.loc['Austria']

   # Create the plot
   fig, ax = plt.subplots(figsize=(8,6))
   ax.plot(x_years, y_gdp, color='darkgreen', linewidth=2, marker='x')
   # etc.
   #+END_SRC

** Plotting multiple data sets
*** Extract values from the DataFrame
#+BEGIN_SRC python
x_years = data.columns
y_austria = data.loc['Austria']
y_bulgaria = data.loc['Bulgaria']
#+END_SRC

*** Create the plot object
#+BEGIN_SRC python
# Create the plot
fig, ax = plt.subplots(figsize=(8,6))
ax.plot(x_years, y_austria, label='Austria', color='darkgreen', linewidth=2, marker='x')
ax.plot(x_years, y_bulgaria, label='Bulgaria', color='maroon', linewidth=2, marker='o')

# Decorate the plot
ax.legend(fontsize=16, loc='upper center') # Uses labels
ax.set_title("GDP of Austria vs Bulgaria", fontsize=22, fontweight='bold')
ax.set_xlabel("Years",fontsize=20, fontweight='bold' )
ax.set_ylabel("GDP",fontsize=20, fontweight='bold' )
#+END_SRC

*** There are many kinds of plots
#+BEGIN_SRC python
plt.style.use('ggplot')

# Create a scatter plot
fig, ax = plt.subplots(figsize=(8,6))
ax.scatter(y_austria, y_bulgaria, color='blue', linewidth=2, marker='o')

# Decorate the plot
ax.set_title("GDP of Austria vs Bulgaria", fontsize=22, fontweight='bold')
ax.set_xlabel("GDP of Austria",fontsize=20, fontweight='bold' )
ax.set_ylabel("GDP of Bulgaria",fontsize=20, fontweight='bold' )
#+END_SRC

*** Overlaying multiple plots on the same figure with Pandas (optional)
This is super unintuitive.
#+BEGIN_SRC python
  # Create an Axes object with the Austria data
  ax = data.loc['Austria'].plot(figsize=(8,6), color='darkgreen', linewidth=2, marker='*')
  print("Austria graph", id(ax))

  # Overlay the Bulgaria data on the same Axes object
  ax = data.loc['Bulgaria'].plot(color='maroon', linewidth=2, marker='o')
  print("Bulgaria graph", id(ax))
#+END_SRC

* COMMENT Seaborn: Pythonic, high-level pre-sets for Matplotlib
** A simple plot
#+BEGIN_SRC python
# Import the Seaborn library
import seaborn as sns
ax = sns.lineplot(data=data.T, legend=False)
#+END_SRC
- Doing more with this data set requires transforming the data from wide form to long form; see https://seaborn.pydata.org/tutorial/data_structure.html

** Import the Iris data set
https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv

#+BEGIN_SRC python
iris = pd.read_csv("data/iris.csv")
iris.head()
#+END_SRC

** Scatter Plot
#+BEGIN_SRC python
# Reset the style
plt.style.use("dark_background")
plt.rcParams["axes.grid"] = False

# Create the plot
ax = sns.scatterplot(data=iris, x='sepal_length',y='petal_length')
#+END_SRC

*** Change plotting theme
#+BEGIN_SRC python
# Make everything visible at a distance
sns.set_context('poster')

# Color by species
ax = sns.scatterplot(data=iris, x='sepal_length', y='petal_length', hue='species', palette='colorblind')

# Set the figure size
fig = ax.get_figure()
fig.set_size_inches(8,6)
#+END_SRC

*** Add styling to data points
#+BEGIN_SRC python
# Color by species
ax = sns.scatterplot(data=iris, x='sepal_length', y='petal_length', hue='species', palette='colorblind', style='species')

# Set the figure size
fig = ax.get_figure()
fig.set_size_inches(8,6)
#+END_SRC

*** Prettify column names (optional)
#+BEGIN_SRC python
words = [' '.join(i) for i in iris.columns.str.split('_')]
iris.columns = words
#+END_SRC

*** Bubble Plot
#+BEGIN_SRC python
# Color by species, size by petal width
ax = sns.scatterplot(data=iris, x='sepal_length', y='petal_length',
                     hue='species', palette='colorblind', size='petal_width')

# (horizontal direction, vertical alignment) of legend
ax.legend(bbox_to_anchor=(1, 1))

# Set the figure size
fig = ax.get_figure()
fig.set_size_inches(8,6)
#+END_SRC

*** Regression Plot (optional)
#+BEGIN_SRC python
# Color by species, size by petal width
ax = sns.regplot(data=iris, x='sepal_length', y='petal_length', scatter=True,
                 scatter_kws={'color':'white'})
#+END_SRC

** Bar Charts
*** Count Plot counts the records in each category
#+BEGIN_SRC python
ax = sns.countplot(data=iris, x='species', palette='colorblind')
#+END_SRC

*** Bar Plot
Default summary statistic is mean, and default error bars are 95% confidence interval.
#+BEGIN_SRC python
ax = sns.barplot(data=iris, x='species', y='sepal_width', palette='colorblind')
#+END_SRC

*** Bar Plot with custom parameters
#+BEGIN_SRC python
# Error bars show standard deviation
ax = sns.barplot(data=iris, x='species', y='sepal_width', ci='sd', edgecolor='black')
#+END_SRC

#+BEGIN_SRC python
# Estimator shows category sum
ax = sns.barplot(data=iris, x='species', y='sepal_width', ci='sd', estimator=np.sum, edgecolor='black')
#+END_SRC

** Histograms
*** Histogram of overall data set
#+BEGIN_SRC python
ax = sns.histplot(data=iris, x='petal_length', kde=True)
#+END_SRC
- KDE: If True, compute a kernel density estimate to smooth the distribution and show on the plot as (one or more) line(s).
- There seems a bimodal distribution of petal length. What factors underly this distribution?

*** Histogram of data decomposed by category
#+BEGIN_SRC python
ax = sns.histplot(data=iris, x='petal_length', hue='species', palette='Set2')
#+END_SRC

*** Selecting number of bins
#+BEGIN_SRC python
# This generates 3 subplots (ncols=3) on the same figure
fig, axes = plt.subplots(figsize=(12,4), nrows=1, ncols=3)
sns.histplot(data=iris,x='petal_length', bins=5, ax=axes[0], color='#f5a142') #  #f5a142 is a hex color
sns.histplot(data=iris,x='petal_length', bins=10, ax=axes[1], color='maroon')
sns.histplot(data=iris,x='petal_length', bins=15, ax=axes[2], color='darkmagenta')
#+END_SRC

** Box Plots and Swarm Plots
*** Basic box plot
#+BEGIN_SRC python
ax = sns.boxplot(data=iris, x='species', y='petal_length')
#+END_SRC

*** Overlap swarm plot
#+BEGIN_SRC python
ax = sns.boxplot(data=iris, x='species', y='petal_length')
sns.swarmplot(data=iris, x='species', y='petal_length', ax=ax, color='black')
#+END_SRC

*** Swarm plot only
#+BEGIN_SRC python
ax = sns.swarmplot(data=iris,x='species', y='petal_length', hue='species', palette='Set1')
ax.legend(loc='upper left', fontsize=16)
ax.tick_params(axis='x', labelrotation = 45)
#+END_SRC

** Heat Map

* COMMENT Seaborn 0.11 new features: https://seaborn.pydata.org/whatsnew.html
* COMMENT Looping through datasets
#+BEGIN_SRC python
  # Saving datasets with new-style string formatting
  for i in datasets_list:
     plt.savefig(f'{i}.png',....)
#+END_SRC

* COMMENT Challenge: Comparing data (rewrite)
Write a program that reads in the regional data sets and plots the average GDP per capita for each region over time in a single chart.

Solution:

#+BEGIN_SRC python
import glob
import pandas as pd
import matplotlib.pyplot as plt
fig, ax = plt.subplots(1,1)
for filename in glob.glob('data/gapminder_gdp*.csv'):
    dataframe = pd.read_csv(filename)
    # extract <region> from the filename, expected to be in the format 'data/gapminder_gdp_<region>.csv'.
    # we will split the string using the split method and `_` as our separator,
    # retrieve the last string in the list that split returns (`<region>.csv`),
    # and then remove the `.csv` extension from that string.
    region = filename.split('_')[-1][:-4]
    dataframe.mean().plot(ax=ax, label=region)
plt.legend()
plt.show()
#+END_SRC

* *Special Topics*
* Working with unstructured files
** Open the file with a context handler
#+BEGIN_SRC python
with open('pettigrew_letters_ORIGINAL.txt', 'r') as file_in:
    text = file_in.read()

print(len(text))
#+END_SRC

** Strings contain formatting marks
Compare the following:
#+BEGIN_SRC python
# This displays the nicely-formatted document
print(text[:300])
#+END_SRC

#+BEGIN_SRC python
# This shows the true nature of the string; you can see newlines (/n),
# tabs (/t), and other hidden characters
text[:300]
#+END_SRC

** Many ways of handling a file
*** ~.read()~ produces the file contents as one string
#+BEGIN_SRC python
type(text)
#+END_SRC

*** ~.readlines()~ produces the file contents as a list of lines; each line is a string
#+BEGIN_SRC python
with open('pettigrew_letters_ORIGINAL.txt', 'r') as file_in:
    text = file_in.readlines()

print(len(text))
print(type(text))
#+END_SRC

*** Inspect parts of the file using list syntax
#+BEGIN_SRC python
# View the first 10 lines
text[:10]
#+END_SRC

** Working with unstructured file data
*** Contents of pettigrew_letters_ORIGINAL.txt
1. Intro material
2. Manifest of letters
3. Individual letters

*** Query: Are all the letters in the manifest actually there?
1. check if all the letters reported in the manifest appear in the actual file
2. check if all the letters in the file are reported in the manifest
3. Therefore, construct two variables: (1) A list of every location line from the manifest, and (2) a list of every location line within the file proper

*** Get the manifest by visual inspection
#+BEGIN_SRC python
manifest_list = text[14:159]
#+END_SRC

*** Use string functions to clean up and inspect text
Demonstrate string tests with manifest_list:
#+BEGIN_SRC python
# Raw text
for location in manifest_list[:10]:
    print(location)
#+END_SRC

#+BEGIN_SRC python
# Remove extra whitespace
for location in manifest_list[:10]:
    print(location.strip())
#+END_SRC

#+BEGIN_SRC python
# Test whether the cleaned line starts with 'Box '
for location in manifest_list[:10]:
    stripped_line = location.strip()
    print(stripped_line.startswith('Box '))
#+END_SRC

#+BEGIN_SRC python
# Test whether the cleaned line starts with 'box '
for location in manifest_list[:10]:
    stripped_line = location.strip()
    print(stripped_line.startswith('box '))
#+END_SRC

*** Gather all the locations in the full document
#+BEGIN_SRC python
letters = text[162:]

for line in letters[:25]:
    # Create a variables to hold current line and truth value of is_box
    stripped_line = line.strip()
    is_box = stripped_line.startswith('Box ')
    if is_box == True:
        print(stripped_line)
    # If the line is empty, don't print anything
    elif stripped_line == '\n':
        continue
    # Indent non-Box lines
    else:
        print('---', stripped_line)
#+END_SRC
- Before automate everything, we run the code with lots of ~print()~ statements so that we can see what's happening

*** Collect the positive results
#+BEGIN_SRC python
letter_locations = []

for line in letters:
    stripped_line = line.strip()
    is_box = stripped_line.startswith("Box ")
    if is_box == True:
        letter_locations.append(stripped_line)
#+END_SRC

*** Compare the manifest and the letters
#+BEGIN_SRC python
print('Items in manifest:', len(manifest_list))
print('Letters:', len(letter_locations))
#+END_SRC

*** Follow-up questions
1. Which items are in one list but not the other?
2. Are there other structural regularities you could use to parse the data? (Note that in the letters, sometimes there are multiple letters under a single box header)

* Exception handling
Explicitly handle common errors, rather than waiting for your code to blow up.
#+BEGIN_SRC python
def average(values):
    "Return average of values, or None if no values are supplied."

    if len(values) == 0:
        return None
    return sum(values) / len(values)

print(average([3, 4, 5]))       # Prints expected output
print(average([]))              # Explicitly handles possible divide-by-zero error
print(average(4))               # Unhandled exception
#+END_SRC

#+BEGIN_SRC python
def average(values):
    "Return average of values, or an informative error if bad values are supplied."

    try:
        return sum(values) / len(values)
    except ZeroDivisionError as err:
        return err
    except TypeError as err:
        return err

print(average([3, 4, 5]))
print(average(4))
print(average([]))
#+END_SRC
- Use judiciously, and be as specific as possible. When in doubt, allow your code to blow up rather than silently commit errors.

* Performance and profiling
#+BEGIN_SRC python
from timeit import time
import cProfile
import pstats

def my_fun(val):
    # Get 1st timestamp
    t1 = time.time()

    # do work

    # Get 2nd timestamp
    t2 = time.time()
    print(round(t2 - t1, 3))

# Run the function with the profiler and collect stats
cProfile.run('my_fun(val)', 'dumpstats')
s = pstats.Stats('dumpstats')
#+END_SRC

* Reducing memory usage
** Read a file one line at a time
#+BEGIN_SRC python
with open('pettigrew_letters_ORIGINAL.txt', 'r') as file_in:
    for line in file_in:
        # Do stuff to current line
        pass
#+END_SRC

** Use a SQLite database
#+BEGIN_SRC python
import sqlite3

conn = sqlite3.connect('my_database_name.db')
with conn:
    c = conn.execute("SELECT column_name FROM table_name WHERE criterion")
    results = c.fetchall()
    c.close

# Do stuff with `results`
#+END_SRC

* Other optional topics
- Checking performance
- List comprehensions
- Defensive programming
- Debugging and Testing

* Credits
- Plotting and Programming in Python (Pandas-oriented): http://swcarpentry.github.io/python-novice-gapminder/
- Programming with Python (NumPy-oriented): https://swcarpentry.github.io/python-novice-inflammation/index.html
- Python for Ecology: https://datacarpentry.org/python-ecology-lesson/
- Humanities Python Tour (file and text processing): https://github.com/elliewix/humanities-python-tour/blob/master/Two-Hour-Beginner-Tour.ipynb
- Introduction to Cultural Analytics & Python: https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html
- Rhondene Wint: Matplotlib and Seaborn notes

* References
- Complete tutorial: https://docs.python.org/3/tutorial/index.html
- Python standard library: https://docs.python.org/3/library/
- Pandas documentation: https://pandas.pydata.org/pandas-docs/stable/
- Pandas user guide: https://pandas.pydata.org/docs/user_guide/index.html
- String formatting: https://pyformat.info/
- True and False in Python: https://docs.python.org/3/library/stdtypes.html#truth-value-testing
- SciPy stats library: https://docs.scipy.org/doc/scipy/reference/stats.html
- Statistics in Python tutorial: https://scipy-lectures.org/packages/statistics/
- Statsmodels library: https://www.statsmodels.org/stable/index.html
- Matplotlib gallery of examples: https://matplotlib.org/gallery/index.html
- Seaborn gallery of examples: https://seaborn.pydata.org/examples/index.html
- IPython magic commands: https://ipython.readthedocs.io/en/stable/interactive/magics.html
- A somewhat-biased comparison of tools for integrating Python with C/C++: http://blog.behnel.de/posts/cython-pybind11-cffi-which-tool-to-choose.html
- How to choose a code editor: https://github.com/elliewix/Ways-Of-Installing-Python/blob/master/ways-of-installing.md#why-do-you-need-a-specific-tool

* Data Sources
1. Gapminder data: http://swcarpentry.github.io/python-novice-gapminder/files/python-novice-gapminder-data.zip
2. Ecology data (field surveys): https://datacarpentry.org/python-ecology-lesson/data/portal-teachingdb-master.zip
3. Social Science data (SAFI): https://datacarpentry.org/socialsci-workshop/data/
4. Humanities data (Pettigrew letters): http://dx.doi.org/10.5334/data.1335350291

* COMMENT How to export this document to other formats
** Export to Markdown using Pandoc
Do this if you want code syntax highlighting and a table of contents on Github.
*** Generate generic Markdown file
#+BEGIN_SRC bash
pandoc README.org -o tmp.md
#+END_SRC

*** Edit generic Markdown file to remove illegal front matter
1. Org directives
2. Anything that isn't part of the document structure (e.g. TODO items)

*** Generate Github Markdown with table of contents
#+BEGIN_SRC bash
pandoc -f markdown --toc --toc-depth=2 -s tmp.md -o README.md
#+END_SRC

*** Find and replace code block markers in final document
#+BEGIN_EXAMPLE
M-x qrr " {.python}" "python"
M-x qrr " {.bash}" "bash"
M-x qrr " example " fundamental
#+END_EXAMPLE

** Export to Markdown using Emacs Org mode
Do this if you want a table of contents on Github.
#+BEGIN_EXAMPLE
M-x org-md-export-to-markdown
#+END_EXAMPLE

** Export to Open Office using Emacs Org mode
#+BEGIN_EXAMPLE
M-x org-odt-export-to-odt
#+END_EXAMPLE

** Export to Microsoft Word using Pandoc
#+BEGIN_SRC bash
# The --reference-doc flag is optional; it provides fine-grained control
# over the appearance of the output document
pandoc README.org -t markdown | pandoc --no-highlight --reference-doc=/Users/gilgamesh/Google Drive/Templates/custom-reference.docx -o README.docx
#+END_SRC
